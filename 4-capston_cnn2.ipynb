{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Learning Assignment 4(convolutional)\n",
    "\n",
    "Previously in 2_fullyconnected.ipynb and 3_regularization.ipynb, we trained fully connected networks to classify notMNIST characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "#from _future_ import print_function   #with this module , should use print(..)\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified notMNIST_large.tar.gz\n",
      "Found and verified notMNIST_small.tar.gz\n"
     ]
    }
   ],
   "source": [
    "url = 'http://yaroslavvb.com/upload/notMNIST/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urllib.urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print 'Found and verified', filename\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'Failed to verify' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['notMNIST_large/A', 'notMNIST_large/B', 'notMNIST_large/C', 'notMNIST_large/D', 'notMNIST_large/E', 'notMNIST_large/F', 'notMNIST_large/G', 'notMNIST_large/H', 'notMNIST_large/I', 'notMNIST_large/J']\n",
      "['notMNIST_small/A', 'notMNIST_small/B', 'notMNIST_small/C', 'notMNIST_small/D', 'notMNIST_small/E', 'notMNIST_small/F', 'notMNIST_small/G', 'notMNIST_small/H', 'notMNIST_small/I', 'notMNIST_small/J']\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "#num_classes = 20\n",
    "\n",
    "def extract(filename):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # removed .tar.gz,such as notMNIST_large\n",
    "  if not os.path.exists(root):    #if not exit notMNIST_large\n",
    "      tar = tarfile.open(filename)\n",
    "      tar.extractall()\n",
    "      tar.close()\n",
    "  data_folders = [os.path.join(root, d) for d in sorted(os.listdir(root))]\n",
    "  if len(data_folders) != num_classes:\n",
    "    raise Exception(\n",
    "      'Expected %d folders, one per class. Found %d instead.' % (\n",
    "        num_classes, len(data_folders)))\n",
    "  print data_folders\n",
    "  return data_folders\n",
    "  \n",
    "train_folders = extract(train_filename)\n",
    "test_folders = extract(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickling notMNIST_large/A.pickle.\n",
      "notMNIST_large/A\n",
      "('Could not read:', 'notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png', ':', IOError(\"cannot identify image file 'notMNIST_large/A/SG90IE11c3RhcmQgQlROIFBvc3Rlci50dGY=.png'\",), \"- it's ok, skipping.\")\n",
      "('Could not read:', 'notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png', ':', IOError(\"cannot identify image file 'notMNIST_large/A/Um9tYW5hIEJvbGQucGZi.png'\",), \"- it's ok, skipping.\")\n",
      "('Could not read:', 'notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png', ':', IOError(\"cannot identify image file 'notMNIST_large/A/RnJlaWdodERpc3BCb29rSXRhbGljLnR0Zg==.png'\",), \"- it's ok, skipping.\")\n",
      "('Full dataset tensor:', (52912, 28, 28))\n",
      "('Mean:', -0.12824284)\n",
      "('Standard deviation:', 0.44310904)\n",
      "Pickling notMNIST_large/B.pickle.\n",
      "notMNIST_large/B\n",
      "('Could not read:', 'notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png', ':', IOError(\"cannot identify image file 'notMNIST_large/B/TmlraXNFRi1TZW1pQm9sZEl0YWxpYy5vdGY=.png'\",), \"- it's ok, skipping.\")\n",
      "('Full dataset tensor:', (52912, 28, 28))\n",
      "('Mean:', -0.0075629)\n",
      "('Standard deviation:', 0.45448747)\n",
      "Pickling notMNIST_large/C.pickle.\n",
      "notMNIST_large/C\n",
      "('Full dataset tensor:', (52912, 28, 28))\n",
      "('Mean:', -0.1422579)\n",
      "('Standard deviation:', 0.43980625)\n",
      "Pickling notMNIST_large/D.pickle.\n",
      "notMNIST_large/D\n",
      "('Could not read:', 'notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png', ':', IOError(\"cannot identify image file 'notMNIST_large/D/VHJhbnNpdCBCb2xkLnR0Zg==.png'\",), \"- it's ok, skipping.\")\n",
      "('Full dataset tensor:', (52912, 28, 28))\n",
      "('Mean:', -0.057366729)\n",
      "('Standard deviation:', 0.45564359)\n",
      "Pickling notMNIST_large/E.pickle.\n",
      "notMNIST_large/E\n",
      "('Full dataset tensor:', (52912, 28, 28))\n",
      "('Mean:', -0.069898896)\n",
      "('Standard deviation:', 0.4529416)\n",
      "Pickling notMNIST_large/F.pickle.\n",
      "notMNIST_large/F\n",
      "('Full dataset tensor:', (52912, 28, 28))\n",
      "('Mean:', -0.12558337)\n",
      "('Standard deviation:', 0.44708973)\n",
      "Pickling notMNIST_large/G.pickle.\n",
      "notMNIST_large/G\n",
      "('Full dataset tensor:', (52912, 28, 28))\n",
      "('Mean:', -0.09458147)\n",
      "('Standard deviation:', 0.44623989)\n",
      "Pickling notMNIST_large/H.pickle.\n",
      "notMNIST_large/H\n",
      "('Full dataset tensor:', (52912, 28, 28))\n",
      "('Mean:', -0.068522125)\n",
      "('Standard deviation:', 0.45423198)\n",
      "Pickling notMNIST_large/I.pickle.\n",
      "notMNIST_large/I\n",
      "('Full dataset tensor:', (52912, 28, 28))\n",
      "('Mean:', 0.03078625)\n",
      "('Standard deviation:', 0.46889836)\n",
      "Pickling notMNIST_large/J.pickle.\n",
      "notMNIST_large/J\n",
      "('Full dataset tensor:', (52911, 28, 28))\n",
      "('Mean:', -0.15335843)\n",
      "('Standard deviation:', 0.44365683)\n",
      "Pickling notMNIST_small/A.pickle.\n",
      "notMNIST_small/A\n",
      "('Could not read:', 'notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png', ':', IOError(\"cannot identify image file 'notMNIST_small/A/RGVtb2NyYXRpY2FCb2xkT2xkc3R5bGUgQm9sZC50dGY=.png'\",), \"- it's ok, skipping.\")\n",
      "('Full dataset tensor:', (1873, 28, 28))\n",
      "('Mean:', -0.13255554)\n",
      "('Standard deviation:', 0.44501966)\n",
      "Pickling notMNIST_small/B.pickle.\n",
      "notMNIST_small/B\n",
      "('Full dataset tensor:', (1873, 28, 28))\n",
      "('Mean:', 0.0053560827)\n",
      "('Standard deviation:', 0.45711541)\n",
      "Pickling notMNIST_small/C.pickle.\n",
      "notMNIST_small/C\n",
      "('Full dataset tensor:', (1873, 28, 28))\n",
      "('Mean:', -0.14152052)\n",
      "('Standard deviation:', 0.4426904)\n",
      "Pickling notMNIST_small/D.pickle.\n",
      "notMNIST_small/D\n",
      "('Full dataset tensor:', (1873, 28, 28))\n",
      "('Mean:', -0.049216654)\n",
      "('Standard deviation:', 0.45975894)\n",
      "Pickling notMNIST_small/E.pickle.\n",
      "notMNIST_small/E\n",
      "('Full dataset tensor:', (1873, 28, 28))\n",
      "('Mean:', -0.059914775)\n",
      "('Standard deviation:', 0.45734972)\n",
      "Pickling notMNIST_small/F.pickle.\n",
      "notMNIST_small/F\n",
      "('Could not read:', 'notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png', ':', IOError(\"cannot identify image file 'notMNIST_small/F/Q3Jvc3NvdmVyIEJvbGRPYmxpcXVlLnR0Zg==.png'\",), \"- it's ok, skipping.\")\n",
      "('Full dataset tensor:', (1873, 28, 28))\n",
      "('Mean:', -0.1182038)\n",
      "('Standard deviation:', 0.45226124)\n",
      "Pickling notMNIST_small/G.pickle.\n",
      "notMNIST_small/G\n",
      "('Full dataset tensor:', (1872, 28, 28))\n",
      "('Mean:', -0.092550322)\n",
      "('Standard deviation:', 0.4490059)\n",
      "Pickling notMNIST_small/H.pickle.\n",
      "notMNIST_small/H\n",
      "('Full dataset tensor:', (1872, 28, 28))\n",
      "('Mean:', -0.058689263)\n",
      "('Standard deviation:', 0.45875889)\n",
      "Pickling notMNIST_small/I.pickle.\n",
      "notMNIST_small/I\n",
      "('Full dataset tensor:', (1872, 28, 28))\n",
      "('Mean:', 0.052645065)\n",
      "('Standard deviation:', 0.47189358)\n",
      "Pickling notMNIST_small/J.pickle.\n",
      "notMNIST_small/J\n",
      "('Full dataset tensor:', (1872, 28, 28))\n",
      "('Mean:', -0.15168923)\n",
      "('Standard deviation:', 0.44801357)\n"
     ]
    }
   ],
   "source": [
    "## udacity versin ..\n",
    "# if error happend such as num_class is 20, delete  two notMNIST_ directory and reexcute this cell.\n",
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "  \"\"\"Load the data for a single letter label.\"\"\"\n",
    "  image_files = os.listdir(folder)\n",
    "  dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "  print(folder)\n",
    "  for image_index, image in enumerate(image_files):\n",
    "    image_file = os.path.join(folder, image)\n",
    "    try:\n",
    "      image_data = (ndimage.imread(image_file).astype(float) - \n",
    "                    pixel_depth / 2) / pixel_depth\n",
    "      if image_data.shape != (image_size, image_size):\n",
    "        raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "      dataset[image_index, :, :] = image_data\n",
    "    except IOError as e:\n",
    "      print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "  num_images = image_index + 1\n",
    "  dataset = dataset[0:num_images, :, :]\n",
    "  if num_images < min_num_images:\n",
    "    raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "  print('Full dataset tensor:', dataset.shape)\n",
    "  print('Mean:', np.mean(dataset))\n",
    "  print('Standard deviation:', np.std(dataset))\n",
    "  return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "  dataset_names = []\n",
    "  for folder in data_folders:\n",
    "    set_filename = folder + '.pickle'\n",
    "    dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "      print('Pickling %s.' % set_filename)\n",
    "      dataset = load_letter(folder, min_num_images_per_class)\n",
    "      try:\n",
    "        with open(set_filename, 'wb') as f:\n",
    "          pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "      except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "  return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge and prune the training data\n",
    "Merge and prune the training data as needed. Depending on your computer setup, you might not be able to fit it all in memory, and you can tune train_size as needed. The labels will be stored into a separate array of integers 0 through 9.\n",
    "Also create a validation dataset for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training:', (200000, 28, 28), (200000,))\n",
      "('Validation:', (10000, 28, 28), (10000,))\n",
      "('Testing:', (18724, 28, 28), (18724,))\n"
     ]
    }
   ],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "  if nb_rows:\n",
    "    dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32) # nb_rows of (img_size by img_size) array\n",
    "    labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "  else:\n",
    "    dataset, labels = None, None\n",
    "  return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "  num_classes = len(pickle_files)\n",
    "  valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "  train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "  vsize_per_class = valid_size // num_classes\n",
    "  tsize_per_class = train_size // num_classes\n",
    "    \n",
    "  start_v, start_t = 0, 0\n",
    "  end_v, end_t = vsize_per_class, tsize_per_class\n",
    "  end_l = vsize_per_class+tsize_per_class\n",
    "  for label, pickle_file in enumerate(pickle_files):       \n",
    "    try:\n",
    "      with open(pickle_file, 'rb') as f:\n",
    "        letter_set = pickle.load(f)\n",
    "        # let's shuffle the letters to have random validation and training set\n",
    "        np.random.shuffle(letter_set)\n",
    "        if valid_dataset is not None:\n",
    "          valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "          valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "          valid_labels[start_v:end_v] = label\n",
    "          start_v += vsize_per_class\n",
    "          end_v += vsize_per_class\n",
    "                    \n",
    "        train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "        train_dataset[start_t:end_t, :, :] = train_letter\n",
    "        train_labels[start_t:end_t] = label\n",
    "        start_t += tsize_per_class\n",
    "        end_t += tsize_per_class\n",
    "    except Exception as e:\n",
    "      print('Unable to process data from', pickle_file, ':', e)\n",
    "      raise\n",
    "    \n",
    "  return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 18724\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "#train_dataset: array consists of 200000 row of (28by28array)\n",
    "#trani_lables : array consist of train_labels: 10000 row of(28by28 array) \n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we'll randomize the data. It's important to have the labels well shuffled for the training and test distributions to match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  save the data for later reuse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "try:\n",
    "  f = open(pickle_file, 'wb')\n",
    "  save = {\n",
    "    'train_dataset': train_dataset,\n",
    "    'train_labels': train_labels,\n",
    "    'valid_dataset': valid_dataset,\n",
    "    'valid_labels': valid_labels,\n",
    "    'test_dataset': test_dataset,\n",
    "    'test_labels': test_labels,\n",
    "    }\n",
    "  pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "  f.close()\n",
    "except Exception as e:\n",
    "  print('Unable to save data to', pickle_file, ':', e)\n",
    "  raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Compressed pickle size:', 718193801)\n"
     ]
    }
   ],
   "source": [
    "statinfo = os.stat(pickle_file)\n",
    "print('Compressed pickle size:', statinfo.st_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (18724, 28, 28) (18724,)\n"
     ]
    }
   ],
   "source": [
    "#### make notMNIST.pickle consists of train_dataset, train_labels, valid_datasets, test_dataset, test_labels.\n",
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print 'Training set', train_dataset.shape, train_labels.shape\n",
    "  print 'Validation set', valid_dataset.shape, valid_labels.shape\n",
    "  print 'Test set', test_dataset.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reformat into a TensorFlow-friendly shape:\n",
    "\n",
    "    convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "    labels as float 1-hot encodings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(200000, 1)\n",
      "(200000, 10)\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "num_labels = 10\n",
    "print(np.arange(num_labels).shape)\n",
    "print(train_labels[:,None].shape)\n",
    "print (((np.arange(num_labels) == train_labels[:,None]).astype(np.float32)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##make ref for multiple number read by softmax()  ###### do not excute !!! \n",
    "# #read the mnist dataset \n",
    "# X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "# Y_ = tf.placeholder(tf.float32, [None, 10]) #correct answers \n",
    "# W = tf.Variable(tf.zeros([784, 10]))# weights \n",
    "# b = tf.Variable(tf.zeros([10])) # biases \n",
    "\n",
    "# Y = tf.nn.softmax(tf.matmul(XX, W) + b) # The model\n",
    "# XX = tf.reshape(X, [-1, 784])\n",
    "# #set the loss function (cross_entropy)\n",
    "# #calculate accuracy (tf.reduce_mean)\n",
    "\n",
    "# #set optimizer\n",
    "# optimizer = tf.train.GradientDescentOptimizer(0.005).minimize(cross_entropy)\n",
    "\n",
    "# sess = tf.Session(tf.initialize_all_variables())\n",
    "# sess.run(init)\n",
    "\n",
    "# #on a loop (n interations):\n",
    "\n",
    "#     outX, outY = mnist.train.next_batch(100)\n",
    "\n",
    "#     #get accuracy and loss on training set\n",
    "#     a, c = sess.run([accuracy, cross_entropy], feed_dict={X: outX, Y_: outY})\n",
    "\t\n",
    "#     #get accuracy and loss on testing set\n",
    "#     a, c = sess.run([accuracy, cross_entropy, ], feed_dict={X: mnist.test.images, Y_: mnist.test.labels})\n",
    "\t\t    \t\t\n",
    "#     #do the backpropagation on training step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (18724, 28, 28, 1) (18724, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape(\n",
    "    (-1, image_size, image_size, num_channels)).astype(np.float32) #4d fm 3d\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32) # 3d but this output is 2d ? check !!\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print 'Training set', train_dataset.shape, train_labels.shape\n",
    "print 'Validation set', valid_dataset.shape, valid_labels.shape\n",
    "print 'Test set', test_dataset.shape, test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LecunLCN(X, image_shape, threshold=1e-4, radius=7, use_divisor=True):\n",
    "    \"\"\"Local Contrast Normalization\"\"\"\n",
    "    \"\"\"[http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf]\"\"\"\n",
    "    \n",
    "#         Allocate an LCN.\n",
    "#         :type X: theano.tensor.dtensor4\n",
    "#         :param X: symbolic image tensor, of shape image_shape\n",
    "#         :type image_shape: tuple or list of length 4\n",
    "#         :param image_shape: (batch size, num input feature maps,\n",
    "#                              image height, image width)\n",
    "#         :type threshold: double\n",
    "#         :param threshold: the threshold will be used to avoid division by zeros\n",
    "#         :type radius: int\n",
    "#         :param radius: determines size of Gaussian filter patch (default 9x9)\n",
    "#         :type use_divisor: Boolean\n",
    "#         :param use_divisor: whether or not to apply divisive normalization\n",
    "    \n",
    "    # Get Gaussian filter\n",
    "    filter_shape = (radius, radius, image_shape[3], 1)\n",
    "\n",
    "    #self.filters = theano.shared(self.gaussian_filter(filter_shape), borrow=True)\n",
    "    filters = gaussian_filter(filter_shape)\n",
    "    X = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    # Compute the Guassian weighted average by means of convolution\n",
    "    convout = tf.nn.conv2d(X, filters, [1,1,1,1], 'SAME')\n",
    "\n",
    "    # Subtractive step\n",
    "    mid = int(np.floor(filter_shape[1] / 2.))\n",
    "\n",
    "    # Make filter dimension broadcastable and subtract\n",
    "    centered_X = tf.sub(X, convout)\n",
    "\n",
    "    # Boolean marks whether or not to perform divisive step\n",
    "    if use_divisor:\n",
    "        # Note that the local variances can be computed by using the centered_X\n",
    "        # tensor. If we convolve this with the mean filter, that should give us\n",
    "        # the variance at each point. We simply take the square root to get our\n",
    "        # denominator\n",
    "\n",
    "        # Compute variances\n",
    "        sum_sqr_XX = tf.nn.conv2d(tf.square(centered_X), filters, [1,1,1,1], 'SAME')\n",
    "\n",
    "        # Take square root to get local standard deviation\n",
    "        denom = tf.sqrt(sum_sqr_XX)\n",
    "\n",
    "        per_img_mean = tf.reduce_mean(denom)\n",
    "        divisor = tf.maximum(per_img_mean, denom)\n",
    "        # Divisise step\n",
    "        new_X = tf.truediv(centered_X, tf.maximum(divisor, threshold))\n",
    "    else:\n",
    "        new_X = centered_X\n",
    "\n",
    "    return new_X\n",
    "\n",
    "\n",
    "def gaussian_filter(kernel_shape):\n",
    "    x = np.zeros(kernel_shape, dtype = float)\n",
    "    mid = np.floor(kernel_shape[0] / 2.)\n",
    "    \n",
    "    for kernel_idx in xrange(0, kernel_shape[2]):\n",
    "        for i in xrange(0, kernel_shape[0]):\n",
    "            for j in xrange(0, kernel_shape[1]):\n",
    "                x[i, j, kernel_idx, 0] = gauss(i - mid, j - mid)\n",
    "    \n",
    "    return tf.convert_to_tensor(x / np.sum(x), dtype=tf.float32)\n",
    "\n",
    "def gauss(x, y, sigma=3.0):\n",
    "    Z = 2 * np.pi * sigma ** 2\n",
    "    return  1. / Z * np.exp(-(x ** 2 + y ** 2) / (2. * sigma ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "num_labels = 11 # 0-9, + blank \n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "batch_size = 64\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "num_hidden1 = 64\n",
    "#num_hidden2 = 16\n",
    "shape = [batch_size, image_size, image_size, num_channels]\n",
    "\n",
    "# Construct a 7-layer CNN.\n",
    "# C1: convolutional layer, batch_size x 28 x 28 x 16, convolution size: 5 x 5 x 1 x 16\n",
    "# S2: sub-sampling layer, batch_size x 14 x 14 x 16\n",
    "# C3: convolutional layer, batch_size x 10 x 10 x 32, convolution size: 5 x 5 x 16 x 32\n",
    "# S4: sub-sampling layer, batch_size x 5 x 5 x 32\n",
    "# C5: convolutional layer, batch_size x 1 x 1 x 64, convolution size: 5 x 5 x 32 x 64\n",
    "# Dropout\n",
    "# F6: fully-connected layer, weight size: 64 x 16\n",
    "# Output layer, weight size: 16 x 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.get_variable(\"W1\", shape=[patch_size, patch_size, num_channels, depth1],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "  layer1_biases = tf.Variable(tf.constant(1.0, shape=[depth1]), name='B1')\n",
    "  layer2_weights = tf.get_variable(\"W2\", shape=[patch_size, patch_size, depth1, depth2],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth2]), name='B2')\n",
    "  layer3_weights = tf.get_variable(\"W3\", shape=[patch_size, patch_size, depth2, num_hidden1],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden1]), name='B3')\n",
    "\n",
    "  s1_w = tf.get_variable(\"WS1\", shape=[num_hidden1, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s1_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS1')\n",
    "  s2_w = tf.get_variable(\"WS2\", shape=[num_hidden1, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s2_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS2')\n",
    "  s3_w = tf.get_variable(\"WS3\", shape=[num_hidden1, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s3_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS3')\n",
    "  s4_w = tf.get_variable(\"WS4\", shape=[num_hidden1, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s4_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS4')\n",
    "  s5_w = tf.get_variable(\"WS5\", shape=[num_hidden1, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s5_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS5')\n",
    "  \n",
    "  # Model.\n",
    "  def model(data, keep_prob, shape):\n",
    "    LCN = LecunLCN(data, shape)\n",
    "    conv = tf.nn.conv2d(LCN, layer1_weights, [1,1,1,1], 'VALID', name='C1')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    lrn = tf.nn.local_response_normalization(hidden)\n",
    "    sub = tf.nn.max_pool(lrn, [1,2,2,1], [1,2,2,1], 'SAME', name='S2')\n",
    "    conv = tf.nn.conv2d(sub, layer2_weights, [1,1,1,1], padding='VALID', name='C3')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    lrn = tf.nn.local_response_normalization(hidden)\n",
    "    sub = tf.nn.max_pool(lrn, [1,2,2,1], [1,2,2,1], 'SAME', name='S4')\n",
    "    conv = tf.nn.conv2d(sub, layer3_weights, [1,1,1,1], padding='VALID', name='C5')\n",
    "    hidden = tf.nn.relu(conv + layer3_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    #hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "    logits1 = tf.matmul(reshape, s1_w) + s1_b\n",
    "    logits2 = tf.matmul(reshape, s2_w) + s2_b\n",
    "    logits3 = tf.matmul(reshape, s3_w) + s3_b\n",
    "    logits4 = tf.matmul(reshape, s4_w) + s4_b\n",
    "    logits5 = tf.matmul(reshape, s5_w) + s5_b\n",
    "    return [logits1, logits2, logits3, logits4, logits5]\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits1, logits2, logits3, logits4, logits5] = model(tf_train_dataset, 0.9375, shape)\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, tf_train_labels[:,1])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits2, tf_train_labels[:,2])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits3, tf_train_labels[:,3])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits4, tf_train_labels[:,4])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits5, tf_train_labels[:,5]))\n",
    "    \n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.AdagradOptimizer(0.01).minimize(loss)\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[0]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[1]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[2]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[3]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[4])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[0]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[1]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[2]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[3]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[4])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[0]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[1]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[2]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[3]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[4])])\n",
    "    \n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()  \n",
    "  reader = tf.train.NewCheckpointReader(\"CNN_1.ckpt\")\n",
    "  reader.get_variable_to_shape_map()\n",
    "  #saver.restore(session, \"CNN_1.ckpt\")\n",
    "  print(\"Model restored.\")  \n",
    "\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size),:]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0): \n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,1:6]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,1:6]))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,1:6]))\n",
    "  save_path = saver.save(session, \"CNN_multi.ckpt\")\n",
    "  print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "num_labels = 11 # 0-9, + blank \n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "batch_size = 64\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "num_hidden1 = 64\n",
    "#num_hidden2 = 16\n",
    "shape = [batch_size, image_size, image_size, num_channels]\n",
    "\n",
    "# Construct a 7-layer CNN.\n",
    "# C1: convolutional layer, batch_size x 28 x 28 x 16, convolution size: 5 x 5 x 1 x 16\n",
    "# S2: sub-sampling layer, batch_size x 14 x 14 x 16\n",
    "# C3: convolutional layer, batch_size x 10 x 10 x 32, convolution size: 5 x 5 x 16 x 32\n",
    "# S4: sub-sampling layer, batch_size x 5 x 5 x 32\n",
    "# C5: convolutional layer, batch_size x 1 x 1 x 64, convolution size: 5 x 5 x 32 x 64\n",
    "# Dropout\n",
    "# F6: fully-connected layer, weight size: 64 x 16\n",
    "# Output layer, weight size: 16 x 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "  tf_train_labels = tf.placeholder(tf.int32, shape=(batch_size, 6))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.get_variable(\"W1\", shape=[patch_size, patch_size, num_channels, depth1],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "  layer1_biases = tf.Variable(tf.constant(1.0, shape=[depth1]), name='B1')\n",
    "  layer2_weights = tf.get_variable(\"W2\", shape=[patch_size, patch_size, depth1, depth2],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth2]), name='B2')\n",
    "  layer3_weights = tf.get_variable(\"W3\", shape=[patch_size, patch_size, depth2, num_hidden1],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden1]), name='B3')\n",
    "\n",
    "  s1_w = tf.get_variable(\"WS1\", shape=[num_hidden1, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s1_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS1')\n",
    "  s2_w = tf.get_variable(\"WS2\", shape=[num_hidden1, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s2_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS2')\n",
    "  s3_w = tf.get_variable(\"WS3\", shape=[num_hidden1, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s3_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS3')\n",
    "  s4_w = tf.get_variable(\"WS4\", shape=[num_hidden1, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s4_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS4')\n",
    "  s5_w = tf.get_variable(\"WS5\", shape=[num_hidden1, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s5_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS5')\n",
    "  \n",
    "  # Model.\n",
    "  def model(data, keep_prob, shape):\n",
    "    LCN = LecunLCN(data, shape)\n",
    "    conv = tf.nn.conv2d(LCN, layer1_weights, [1,1,1,1], 'VALID', name='C1')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    lrn = tf.nn.local_response_normalization(hidden)\n",
    "    sub = tf.nn.max_pool(lrn, [1,2,2,1], [1,2,2,1], 'SAME', name='S2')\n",
    "    conv = tf.nn.conv2d(sub, layer2_weights, [1,1,1,1], padding='VALID', name='C3')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    lrn = tf.nn.local_response_normalization(hidden)\n",
    "    sub = tf.nn.max_pool(lrn, [1,2,2,1], [1,2,2,1], 'SAME', name='S4')\n",
    "    conv = tf.nn.conv2d(sub, layer3_weights, [1,1,1,1], padding='VALID', name='C5')\n",
    "    hidden = tf.nn.relu(conv + layer3_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    #hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "    logits1 = tf.matmul(reshape, s1_w) + s1_b\n",
    "    logits2 = tf.matmul(reshape, s2_w) + s2_b\n",
    "    logits3 = tf.matmul(reshape, s3_w) + s3_b\n",
    "    logits4 = tf.matmul(reshape, s4_w) + s4_b\n",
    "    logits5 = tf.matmul(reshape, s5_w) + s5_b\n",
    "    return [logits1, logits2, logits3, logits4, logits5]\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits1, logits2, logits3, logits4, logits5] = model(tf_train_dataset, 0.875, shape)\n",
    "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits1, tf_train_labels[:,1])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits2, tf_train_labels[:,2])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits3, tf_train_labels[:,3])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits4, tf_train_labels[:,4])) +\\\n",
    "tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits5, tf_train_labels[:,5]))\n",
    "    \n",
    "  # Optimizer.\n",
    "  #optimizer = tf.train.AdagradOptimizer(0.01).minimize(loss)\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(0.05, global_step, 10000, 0.95)\n",
    "  optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.pack([tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[0]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[1]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[2]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[3]),\\\n",
    "                      tf.nn.softmax(model(tf_train_dataset, 1.0, shape)[4])])\n",
    "  valid_prediction = tf.pack([tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[0]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[1]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[2]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[3]),\\\n",
    "                      tf.nn.softmax(model(tf_valid_dataset, 1.0, shape)[4])])\n",
    "  test_prediction = tf.pack([tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[0]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[1]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[2]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[3]),\\\n",
    "                     tf.nn.softmax(model(tf_test_dataset, 1.0, shape)[4])])\n",
    "    \n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  saver.restore(session, \"CNN_multi2.ckpt\")\n",
    "  print(\"Model restored.\")  \n",
    "\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size),:]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 200 == 0): \n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels[:,1:6]))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels[:,1:6]))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels[:,1:6]))\n",
    "  save_path = saver.save(session, \"CNN_multi2.ckpt\")\n",
    "  print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (20.0, 20.0)\n",
    "f, ax = plt.subplots(nrows=1, ncols=10)\n",
    "\n",
    "im_samples = []\n",
    "    \n",
    "for i, j in enumerate(np.sort(np.random.randint(0, test_labels.shape[0], size=10))):\n",
    "    filename = str(j+1)+'.png'\n",
    "    fullname = os.path.join('test', filename)\n",
    "    im = Image.open(fullname)\n",
    "    house_num = ''\n",
    "    for k in np.arange(test_labels[j,0]):\n",
    "        house_num += str(test_labels[j,k+1])\n",
    "    im_samples.extend([j])\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(house_num, loc='center')\n",
    "    ax[i].imshow(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "num_labels = 11 # 0-9, + blank \n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "patch_size = 5\n",
    "depth1 = 16\n",
    "depth2 = 32\n",
    "depth3 = 64\n",
    "num_hidden1 = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  tf_test_dataset = tf.placeholder(tf.float32, shape=(10, 32, 32, 1))\n",
    "  \n",
    "  # Variables.\n",
    "  layer1_weights = tf.get_variable(\"W1\", shape=[patch_size, patch_size, num_channels, depth1],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "  layer1_biases = tf.Variable(tf.constant(1.0, shape=[depth1]), name='B1')\n",
    "  layer2_weights = tf.get_variable(\"W2\", shape=[patch_size, patch_size, depth1, depth2],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "  layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth2]), name='B2')\n",
    "  layer3_weights = tf.get_variable(\"W3\", shape=[patch_size, patch_size, depth2, num_hidden1],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "  layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden1]), name='B3')\n",
    "\n",
    "  s1_w = tf.get_variable(\"WS1\", shape=[num_hidden1, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s1_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS1')\n",
    "  s2_w = tf.get_variable(\"WS2\", shape=[num_hidden1, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s2_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS2')\n",
    "  s3_w = tf.get_variable(\"WS3\", shape=[num_hidden1, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s3_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS3')\n",
    "  s4_w = tf.get_variable(\"WS4\", shape=[num_hidden1, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s4_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS4')\n",
    "  s5_w = tf.get_variable(\"WS5\", shape=[num_hidden1, num_labels],\\\n",
    "           initializer=tf.contrib.layers.xavier_initializer())\n",
    "  s5_b = tf.Variable(tf.constant(1.0, shape=[num_labels]), name='BS5')\n",
    "  \n",
    "  # Model.\n",
    "  def model(data, keep_prob, shape):\n",
    "    LCN = LecunLCN(data, shape)\n",
    "    conv = tf.nn.conv2d(LCN, layer1_weights, [1,1,1,1], 'VALID', name='C1')\n",
    "    hidden = tf.nn.relu(conv + layer1_biases)\n",
    "    lrn = tf.nn.local_response_normalization(hidden)\n",
    "    sub = tf.nn.max_pool(lrn, [1,2,2,1], [1,2,2,1], 'SAME', name='S2')\n",
    "    conv = tf.nn.conv2d(sub, layer2_weights, [1,1,1,1], padding='VALID', name='C3')\n",
    "    hidden = tf.nn.relu(conv + layer2_biases)\n",
    "    lrn = tf.nn.local_response_normalization(hidden)\n",
    "    sub = tf.nn.max_pool(lrn, [1,2,2,1], [1,2,2,1], 'SAME', name='S4')\n",
    "    conv = tf.nn.conv2d(sub, layer3_weights, [1,1,1,1], padding='VALID', name='C5')\n",
    "    hidden = tf.nn.relu(conv + layer3_biases)\n",
    "    hidden = tf.nn.dropout(hidden, keep_prob)\n",
    "    shape = hidden.get_shape().as_list()\n",
    "    reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "    #hidden = tf.nn.relu(tf.matmul(reshape, layer4_weights) + layer4_biases)\n",
    "    logits1 = tf.matmul(reshape, s1_w) + s1_b\n",
    "    logits2 = tf.matmul(reshape, s2_w) + s2_b\n",
    "    logits3 = tf.matmul(reshape, s3_w) + s3_b\n",
    "    logits4 = tf.matmul(reshape, s4_w) + s4_b\n",
    "    logits5 = tf.matmul(reshape, s5_w) + s5_b\n",
    "    return [logits1, logits2, logits3, logits4, logits5]\n",
    "  \n",
    "  # Training computation.\n",
    "  [logits1, logits2, logits3, logits4, logits5] = model(tf_test_dataset, 1, [10, 32, 32, 1])\n",
    "\n",
    "  predict = tf.pack([tf.nn.softmax(logits1),tf.nn.softmax(logits2),tf.nn.softmax(logits3),\\\n",
    "                         tf.nn.softmax(logits4),tf.nn.softmax(logits5)])\n",
    "  test_prediction = tf.transpose(tf.argmax(predict, 2))\n",
    "  saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  saver.restore(session, \"CNN_multi2.ckpt\")\n",
    "  print(\"Model restored.\")  \n",
    "\n",
    "  print('Initialized')\n",
    "  test_prediction = session.run(test_prediction, feed_dict={tf_test_dataset : test_dataset[im_samples,:,:,:],})\n",
    "  print(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (15.0, 15.0)\n",
    "f, ax = plt.subplots(nrows=1, ncols=10)\n",
    "\n",
    "for i, j in enumerate(im_samples):\n",
    "    filename = str(j+1)+'.png'\n",
    "    fullname = os.path.join('test', filename)\n",
    "    im = Image.open(fullname)\n",
    "    house_num = ''\n",
    "    for k in np.arange(np.sum(test_prediction[i,:] != 10)):\n",
    "        house_num += str(test_prediction[i,k])\n",
    "\n",
    "    ax[i].axis('off')\n",
    "    ax[i].set_title(house_num, loc='center')\n",
    "    ax[i].imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #batch_size = 16 #reduced from 128\n",
    "# batch_size = 10 #reduced from 128  #for minibatch in mind.\n",
    "# patch_size = 5  #reduced from image_size,28\n",
    "# #depth = 16\n",
    "# depth = 16\n",
    "# num_hidden = 64 #num_hidden,1024, num_hidden_1( _2):2048\n",
    "\n",
    "# graph = tf.Graph()\n",
    "\n",
    "# with graph.as_default():\n",
    "\n",
    "#   # Input data.\n",
    "#   tf_train_dataset = tf.placeholder(\n",
    "#     tf.float32, shape=(batch_size, image_size, image_size, num_channels))  #4d\n",
    "#   tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))  #2d?\n",
    "#   tf_valid_dataset = tf.constant(valid_dataset)\n",
    "#   tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "#   # Variables.  #path_size used be image_size in previous file\n",
    "#   layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "#       [patch_size, patch_size, num_channels, depth], stddev=0.1))  #4d  , depth used be num_labels\n",
    "#   layer1_biases = tf.Variable(tf.zeros([depth]))  #depth used be num_labels\n",
    "#   layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "#       [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "#   layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "#   layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "#       [image_size / 4 * image_size / 4 * depth, num_hidden], stddev=0.1))\n",
    "#   layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "#   layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "#       [num_hidden, num_labels], stddev=0.1))\n",
    "#   layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "#   # Model.\n",
    "#   def model(data):\n",
    "#     conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "#     hidden = tf.nn.relu(conv + layer1_biases)\n",
    "#     conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "#     hidden = tf.nn.relu(conv + layer2_biases)\n",
    "#     shape = hidden.get_shape().as_list()\n",
    "#     reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "#     hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "#     return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "#   # Training computation.\n",
    "#   logits = model(tf_train_dataset)\n",
    "#   loss = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "#   # Optimizer.\n",
    "#   optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "#   # Predictions for the training, validation, and test data.\n",
    "#   train_prediction = tf.nn.softmax(logits)\n",
    "#   valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "#   test_prediction = tf.nn.softmax(model(tf_test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 3.74318\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 10.0%\n",
      "Minibatch loss at step 50 : 1.47668\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 45.9%\n",
      "Minibatch loss at step 100 : 0.723571\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 64.1%\n",
      "Minibatch loss at step 150 : 0.890883\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 72.2%\n",
      "Minibatch loss at step 200 : 0.792864\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 250 : 0.497681\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 75.5%\n",
      "Minibatch loss at step 300 : 0.304302\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 350 : 0.500182\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 79.2%\n",
      "Minibatch loss at step 400 : 0.872817\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 79.4%\n",
      "Minibatch loss at step 450 : 1.28569\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 500 : 1.20134\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 550 : 0.805278\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 600 : 0.587357\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 650 : 0.988351\n",
      "Minibatch accuracy: 60.0%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 700 : 0.757775\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 750 : 0.598996\n",
      "Minibatch accuracy: 80.0%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 800 : 0.525714\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 850 : 0.917413\n",
      "Minibatch accuracy: 70.0%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 900 : 0.236722\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 950 : 0.343973\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 1000 : 0.358132\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 82.8%\n",
      "Test accuracy: 89.7%\n"
     ]
    }
   ],
   "source": [
    "# num_steps = 1001\n",
    "\n",
    "# with tf.Session(graph=graph) as session:\n",
    "#   tf.initialize_all_variables().run()\n",
    "#   print \"Initialized\"\n",
    "#   for step in xrange(num_steps):\n",
    "#     offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "#     batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "#     batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "#     feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "#     _, l, predictions = session.run(\n",
    "#       [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "#     if (step % 50 == 0):\n",
    "#       print \"Minibatch loss at step\", step, \":\", l\n",
    "#       print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "#       print \"Validation accuracy: %.1f%%\" % accuracy(\n",
    "#         valid_prediction.eval(), valid_labels)\n",
    "#   print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides a <span style=\"color:red\">max pooling</span> operation (nn.max_pool()) of stride 2 and kernel size 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0 : 3.11668\n",
      "Minibatch accuracy: 10.9%\n",
      "Validation accuracy: 11.3%\n",
      "Minibatch loss at step 50 : 2.11099\n",
      "Minibatch accuracy: 22.7%\n",
      "Validation accuracy: 51.7%\n",
      "Minibatch loss at step 100 : 1.4357\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 74.8%\n",
      "Minibatch loss at step 150 : 1.0463\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 78.5%\n",
      "Minibatch loss at step 200 : 1.04754\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 80.2%\n",
      "Minibatch loss at step 250 : 0.967472\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 300 : 0.976286\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 350 : 0.838838\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 400 : 0.89662\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 450 : 0.69273\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 500 : 0.656299\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 550 : 0.809507\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 600 : 0.83232\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 650 : 0.757874\n",
      "Minibatch accuracy: 73.4%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 700 : 0.687784\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 750 : 0.643225\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 800 : 0.68403\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 850 : 0.717426\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 900 : 0.887499\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 950 : 0.619775\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 1000 : 1.02008\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 85.4%\n",
      "Minibatch loss at step 1050 : 0.561971\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 1100 : 0.745256\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 85.7%\n",
      "Minibatch loss at step 1150 : 0.527114\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 1200 : 0.611992\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 1250 : 0.452259\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 1300 : 0.534212\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 1350 : 0.666321\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 1400 : 0.736712\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 1450 : 0.698983\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 1500 : 0.574712\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 1550 : 0.572172\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 1600 : 0.54915\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 1650 : 0.670092\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 1700 : 0.554442\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 1750 : 0.517829\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 1800 : 0.613773\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 1850 : 0.591377\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 1900 : 0.618052\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 1950 : 0.551289\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 2000 : 0.558125\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 2050 : 0.568412\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 2100 : 0.424406\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2150 : 0.559401\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 2200 : 0.717992\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2250 : 0.674774\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2300 : 0.607018\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 2350 : 0.730088\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2400 : 0.47851\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 87.2%\n",
      "Minibatch loss at step 2450 : 0.582993\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2500 : 0.413069\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2550 : 0.567574\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 2600 : 0.558509\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2650 : 0.609888\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 87.3%\n",
      "Minibatch loss at step 2700 : 0.546856\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.4%\n",
      "Minibatch loss at step 2750 : 0.64891\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 2800 : 0.742026\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 87.8%\n",
      "Minibatch loss at step 2850 : 0.53828\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 2900 : 0.475682\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 87.5%\n",
      "Minibatch loss at step 2950 : 0.764208\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 87.7%\n",
      "Minibatch loss at step 3000 : 0.627272\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 87.6%\n",
      "Test accuracy: 93.8%\n"
     ]
    }
   ],
   "source": [
    "# batch_size = 128\n",
    "# patch_size = 5\n",
    "# depth = 16\n",
    "# num_hidden = 64\n",
    "\n",
    "# graph = tf.Graph()\n",
    "\n",
    "# with graph.as_default():\n",
    "\n",
    "#   # Input data.\n",
    "#   tf_train_dataset = tf.placeholder(\n",
    "#     tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "#   tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "#   tf_valid_dataset = tf.constant(valid_dataset)\n",
    "#   tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "#   # Variables.\n",
    "#   layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "#       [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "#   layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "#   layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "#       [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "#   layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "#   layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "#       [image_size / 4 * image_size / 4 * depth, num_hidden], stddev=0.1))\n",
    "#   layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "#   layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "#       [num_hidden, num_labels], stddev=0.1))\n",
    "#   layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "#   # Model.\n",
    "#   def model_pool(data, train=False):\n",
    "#     conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     hidden = tf.nn.relu(conv + layer1_biases)\n",
    "#     pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "#     conv = tf.nn.conv2d(pool, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "#     hidden = tf.nn.relu(conv + layer2_biases)\n",
    "#     pool = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "    \n",
    "#     shape = pool.get_shape().as_list()\n",
    "#     reshape = tf.reshape(pool, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "#     hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "#     if train:\n",
    "#       hidden = tf.nn.dropout(hidden, 0.5)\n",
    "\n",
    "#     return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "#   # Training computation.\n",
    "#   logits = model_pool(tf_train_dataset, True)\n",
    "#   loss = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "   \n",
    "#   # adding regularizers\n",
    "#   regularizers = (tf.nn.l2_loss(layer1_weights) + tf.nn.l2_loss(layer1_biases) +\n",
    "#                   tf.nn.l2_loss(layer2_weights) + tf.nn.l2_loss(layer2_biases) +\n",
    "#                   tf.nn.l2_loss(layer3_weights) + tf.nn.l2_loss(layer3_biases) +\n",
    "#                   tf.nn.l2_loss(layer4_weights) + tf.nn.l2_loss(layer4_biases)\n",
    "#                  )\n",
    "#   # Add the regularization term to the loss.\n",
    "#   loss += 3e-4 * regularizers\n",
    "  \n",
    "#   # Optimizer: set up a variable that's incremented once per batch and\n",
    "#   # controls the learning rate decay.\n",
    "#   batch = tf.Variable(0)\n",
    "#   # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "#   learning_rate = tf.train.exponential_decay(\n",
    "#       0.01,                # Base learning rate.\n",
    "#       batch * batch_size,  # Current index into the dataset.\n",
    "#       train_labels.shape[0],          # Decay step.\n",
    "#       0.95,                # Decay rate.\n",
    "#       staircase=True)\n",
    "#   # Use simple momentum for the optimization.\n",
    "#   optimizer = tf.train.MomentumOptimizer(learning_rate,\n",
    "#                                          0.9).minimize(loss,\n",
    "#                                                        global_step=batch)\n",
    "   \n",
    "#   # Predictions for the training, validation, and test data.\n",
    "#   train_prediction = tf.nn.softmax(logits)\n",
    "#   valid_prediction = tf.nn.softmax(model_pool(tf_valid_dataset))\n",
    "#   test_prediction = tf.nn.softmax(model_pool(tf_test_dataset))\n",
    "# # In[ ]:\n",
    "\n",
    "# num_steps = 3001\n",
    "\n",
    "# with tf.Session(graph=graph) as session:\n",
    "#   tf.initialize_all_variables().run()\n",
    "#   print \"Initialized\"\n",
    "#   for step in xrange(num_steps):\n",
    "#     offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "#     batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "#     batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "#     feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "#     _, l, predictions = session.run(\n",
    "#       [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "#     if (step % 50 == 0):\n",
    "#       print \"Minibatch loss at step\", step, \":\", l\n",
    "#       print \"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels)\n",
    "#       print \"Validation accuracy: %.1f%%\" % accuracy(\n",
    "#         valid_prediction.eval(), valid_labels)\n",
    "#   print \"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
